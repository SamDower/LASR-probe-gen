{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577ea4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Run to set environment variables if want to\n",
    "# %env HF_TOKEN="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a99d188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global imports\n",
    "from probe_gen.paths import data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9d44a5",
   "metadata": {},
   "source": [
    "# Generate and save perplexities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca8ac2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from probe_gen.config import MODELS\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"llama_3b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODELS[model_name])\n",
    "model = AutoModelForCausalLM.from_pretrained(MODELS[model_name], device_map=\"auto\")\n",
    "# If no pad token is set, reuse the EOS token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0177333d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from probe_gen.analysis import calculate_response_perplexities_batched, save_perplexities\n",
    "from probe_gen.config import ACTIVATION_DATASETS\n",
    "from probe_gen.gen_data.utils import load_jsonl_data, format_prompts_from_strings\n",
    "\n",
    "# Get perplexity data\n",
    "behaviour = 'sychophancy'\n",
    "for dataset_type in [f'{model_name}_4k', f'{model_name}_prompted_4k', f'qwen_3b_4k']:\n",
    "# for dataset_type in [f'{model_name}_5k', f'{model_name}_prompted_5k', f'ministral_8b_5k']:\n",
    "    dataset_name = ACTIVATION_DATASETS[f\"{behaviour}_{dataset_type}\"][\"labels_filename\"]\n",
    "    human_list, assistant_list, _ = load_jsonl_data(dataset_name)\n",
    "    formatted_prompts = format_prompts_from_strings(tokenizer, human_list)\n",
    "    perplexities = calculate_response_perplexities_batched(model, tokenizer, formatted_prompts, assistant_list, batch_size=16)\n",
    "    save_perplexities(perplexities, behaviour, model_name, dataset_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58254be3",
   "metadata": {},
   "source": [
    "# Load and visualize perplexities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71281600",
   "metadata": {},
   "outputs": [],
   "source": [
    "from probe_gen.analysis import load_perplexities\n",
    "\n",
    "model_name = 'llama_3b'\n",
    "behaviour = 'science'\n",
    "perplexities_on = load_perplexities(behaviour, model_name, f'{model_name}_5k')\n",
    "perplexities_on_prompted = load_perplexities(behaviour, model_name, f'{model_name}_prompted_5k')\n",
    "perplexities_off = load_perplexities(behaviour, model_name, f'qwen_3b_5k')\n",
    "# perplexities_off = load_perplexities(behaviour, model_name, f'ministral_8b_5k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a31001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Filter out empty responses in dataset generation pipeline instead of here\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def filter_out_nans(data):\n",
    "    arr = np.array(data)\n",
    "    arr = arr[np.isfinite(arr)]\n",
    "    if len(arr) != len(data):\n",
    "        print(f\"Had to reduce data from length {len(data)} to {len(arr)} to filter out NaNs.\")\n",
    "    return arr\n",
    "\n",
    "perplexities_on = filter_out_nans(perplexities_on)\n",
    "perplexities_on_prompted = filter_out_nans(perplexities_on_prompted)\n",
    "perplexities_off = filter_out_nans(perplexities_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d391ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from probe_gen.analysis import plot_perplexities\n",
    "\n",
    "plot_perplexities(\n",
    "    [perplexities_on, perplexities_on_prompted, perplexities_off], \n",
    "    ['On', 'On prompted', 'Off'], \n",
    "    remove_outliers=True, \n",
    "    num_bins=60\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (uv)",
   "language": "python",
   "name": "uv-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
